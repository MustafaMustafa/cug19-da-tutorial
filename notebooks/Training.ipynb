{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-node training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick up the local code\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Externals\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Locals\n",
    "from data import get_datasets\n",
    "from models import get_model\n",
    "from utils.device import configure_session\n",
    "from utils.optimizers import get_optimizer\n",
    "from utils.callbacks import TimingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will be doing image classification on CIFAR10:\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "We will be training a simple CNN model to classify small images into 10 classes.\n",
    "\n",
    "Let's start by looking at some example images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = cifar10.load_data()[0]\n",
    "\n",
    "nrows = 8\n",
    "ncols = 8\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 12), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(x[i])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Here is where we will specify all of our configuration of the model, optimizer, and training procedure.\n",
    "\n",
    "I'll also set some hardware related settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "model_name = 'cnn'\n",
    "input_shape = [32, 32, 3]\n",
    "n_classes = 10\n",
    "dropout = 0.1\n",
    "\n",
    "# Optimizer config\n",
    "optimizer_name = 'Adam'\n",
    "lr = 0.001\n",
    "\n",
    "# Training config\n",
    "batch_size = 32\n",
    "n_epochs = 32\n",
    "loss_name = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware config\n",
    "gpu = 0\n",
    "intra_threads = 2 # CPU performance knob\n",
    "inter_threads = 2 # CPU performance knob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_session(gpu=gpu, intra_threads=intra_threads, inter_threads=inter_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Here we use the local get_datasets code to retrieve and configure our data.\n",
    "\n",
    "This actually returns data generator iterators which can read data and perform on-the-fly augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_gen, valid_gen = get_datasets(name='cifar10', batch_size=batch_size)\n",
    "train_steps = len(train_gen) // batch_size\n",
    "valid_steps = len(valid_gen) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "Now we use our local get_model code to build our CNN model according to our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model and optimizer\n",
    "model = get_model(name=model_name, input_shape=input_shape, n_classes=n_classes, dropout=dropout)\n",
    "opt = get_optimizer(name=optimizer_name, lr=lr, n_ranks=1)\n",
    "model.compile(loss=loss_name, optimizer=opt, metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We use the fit_generator method to train our CNN model on our data generators.\n",
    "\n",
    "Watch the progess as our model eats through the training data and regularly evaluates on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit_generator(train_gen,\n",
    "                              epochs=n_epochs,\n",
    "                              steps_per_epoch=train_steps,\n",
    "                              validation_data=valid_gen,\n",
    "                              validation_steps=valid_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training history\n",
    "\n",
    "Training has now completed. We can use the returned history object to make plots of the training and validation set losses and accuracies during training. This is very valuable for identifying issues like under/over fitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# Plot the loss\n",
    "ax0.plot(history.epoch, history.history['loss'], label='train')\n",
    "ax0.plot(history.epoch, history.history['val_loss'], label='validation')\n",
    "ax0.set_xlabel('Epoch')\n",
    "ax0.set_ylabel('Loss')\n",
    "ax0.legend(loc=0)\n",
    "\n",
    "# Plot the accuracy\n",
    "ax1.plot(history.epoch, history.history['acc'], label='train')\n",
    "ax1.plot(history.epoch, history.history['val_acc'], label='validation')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(bottom=0, top=1)\n",
    "ax1.legend(loc=0)\n",
    "\n",
    "#prettify layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to try next?\n",
    "\n",
    "Now that you've gotten this far, familiarize yourself with what you can change in the configuration and the effect it has on model performance.\n",
    "\n",
    "For example:\n",
    "- What happens if you increase or decrease the learning rate by a factor of 10?\n",
    "- What happens if you greatly increase or decrease the size of the model in number and size of layers?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-1.13.0-py36-dev",
   "language": "python",
   "name": "tensorflow-1.13.0-py36-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
